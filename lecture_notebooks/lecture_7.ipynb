{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH 652: Advanced Numerical Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "d5sMJY4v8Tol"
   },
   "source": [
    "### Topics\n",
    "\n",
    "* Solvers and numerical linear algebra\n",
    "* Conjugate gradient method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Many of the problems we have considered until now in this course have resulted in **symmetric positive-definite** (SPD) matrices.\n",
    "In fact, let $a(\\cdot, \\cdot)$ denote a symmetric, coercive bilinear form, and let $\\{ \\phi_i \\}$ denote a basis for the discrete space $V_h$.\n",
    "Then, the system matrix $A$ defined by\n",
    "$$\n",
    "   A_{ij} = a(\\phi_i, \\phi_j)\n",
    "$$\n",
    "is symmetric and positive-definite (why?)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last term, we looked at the Gauss-Seidel method, which is a simple iterative method for approximating the solution to linear systems\n",
    "$$\n",
    "   A x = b\n",
    "$$\n",
    "and is guaranteed to converge when $A$ is SPD.\n",
    "\n",
    "The convergence of Gauss-Seidel is determined by the matrix norm of the iteration matrix.\n",
    "We have shown that when $A$ is SPD then this matrix norm is strictly less than 1, and so the method converges.\n",
    "However, we have not derived any estimates for **how quickly** Gauss-Seidel will converge (in terms of discretization parameters).\n",
    "We did see last term that as the problem size grew (we refined our mesh), solving a system with the diffusion \"stiffness matrix\" required more iterations, however solving a system with the mass matrix (i.e. corresponding to the L2 inner product) required roughly a constant number of iterations even on more refined problems.\n",
    "\n",
    "* And are there better methods that result in faster convergence than Gauss-Seidel?\n",
    "* Can we quantify or analyze the convergence with respect to discretization paramters?\n",
    "\n",
    "These questions will motivate our study of the **conjugate gradient method**, which is one of the most well-known and widely used **Krylov methods**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system we are interested in solving is\n",
    "$$\n",
    "   Ax = b.\n",
    "$$\n",
    "\n",
    "Define the **quadratic form**\n",
    "$$\n",
    "   f(x) = \\frac{1}{2} x^T A x - b^T x.\n",
    "$$\n",
    "\n",
    "Note that if $x$ is the solution (i.e. $Ax = b$), and $y$ is arbitrary, then\n",
    "$$\n",
    "   \\begin{aligned}\n",
    "      f(x + y) &= \\frac{1}{2} (x^T + y^T) A (x + y) - b^T (x + y) \\\\\n",
    "         &= \\frac{1}{2} x^T A x + x^T A y + \\frac{1}{2} y^T A y - b^T x - b^T y \\\\\n",
    "         &= f(x) + \\frac{1}{2} y^T A y,\n",
    "   \\end{aligned}\n",
    "$$\n",
    "which implies that $x$ **minimizes the quadratic form**.\n",
    "\n",
    "We can also see this as follows.\n",
    "The gradient of $f$ is given by\n",
    "$$\n",
    "   f'(x) = \\begin{pmatrix}\n",
    "      \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "      \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "      \\vdots \\\\\n",
    "      \\frac{\\partial f}{\\partial x_n}\n",
    "   \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The form $f$ will reach its minimum when $f'$ is equal to zero.\n",
    "It is relatively straightforward to see that\n",
    "$$\n",
    "   f'(x) = \\frac{1}{2}A^t x + \\frac{1}{2} A x - b = Ax - b,\n",
    "$$\n",
    "by symmetry.\n",
    "So it is clear that $f$ has a critical point at the solution $x$.\n",
    "When $A$ is positive-definite, the critical point is a minimum.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steepest Descent\n",
    "\n",
    "We can try to minimize the form using a method known as steepest descent.\n",
    "Note that the gradient $f'(x)$ points in the direction of the steepest increase in $f$.\n",
    "So, we can move in the opposite direction to try to find a minimum.\n",
    "\n",
    "Some notation (familiar from before). The **error** $e_{(i)}$ is\n",
    "$$\n",
    "   e_{(i)} = x_{(i)} - x,\n",
    "$$\n",
    "and the **residual** is\n",
    "$$\n",
    "   r_{(i)} = -A e_{(i)} = b - A x_{(i)}.\n",
    "$$\n",
    "\n",
    "Note that the residual is\n",
    "$$\n",
    "   r_{(i)} = -f'(x_{(i)})\n",
    "$$\n",
    "so the residual is pointing in the direction of steepest descent.\n",
    "\n",
    "Knowing the direction of steepest descent allows us to perform a **line search**.\n",
    "A line search is a procedure that will choose the best value long a particular line, i.e. find the best $\\alpha$ to define\n",
    "$$\n",
    "   x_{(i+1)} = x_{(i)} + \\alpha r_{(i)}.\n",
    "$$\n",
    "This is a one-dimensional problem.\n",
    "We can minimize $f$ along this line when\n",
    "$$\n",
    "   \\frac{d}{d\\alpha} f(x_{(i+1)}) = 0.\n",
    "$$\n",
    "By the chain rule\n",
    "$$\n",
    "   \\frac{d}{d\\alpha} f(x_{(i+1)}) = f'(x_{(i+1)})^T \\frac{d}{d\\alpha} x_{(i+1)} = f'(x_{(i+1)})^T r_{(i)},\n",
    "$$\n",
    "and this is zero when $f'(x_{(i+1)})$ is orthogonal to $r_{(i)}$.\n",
    "\n",
    "This is intuitively clear, because the rate of increase or decrease of $f$ along this line is given by the **projection** of the gradient $f'$ onto the search line.\n",
    "When the projection is zero, the rate of increase or decrease along the line is zero, and this happens when the gradient is orthogonal to the search direction.\n",
    "\n",
    "We now determine $\\alpha$.\n",
    "Note that\n",
    "$$\n",
    "   f'(x_{(i+1)}) = r_{(i+1)},\n",
    "$$\n",
    "so we want\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   r_{(i+1)}^T r_{(i)} &= 0 \\\\\n",
    "   (b - Ax_{(i+1)})^T r_{(i)} &= 0 \\\\\n",
    "   (b - A(x_{(i)} + \\alpha r_{(i)}))^T r_{(i)} &= 0 \\\\\n",
    "   (b - Ax_{(i)})^T r_{(i)} - \\alpha (A r_{(i)})^T r_{(i)} &= 0 \\\\\n",
    "   r_{(i)}^T r_{(i)} - \\alpha (A r_{(i)})^T r_{(i)} &= 0 \\\\\n",
    "   \\alpha (A r_{(i)})^T r_{(i)} &= r_{(i)}^T r_{(i)} \\\\\n",
    "   \\alpha &= \\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T A r_{(i)}} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together, we obtain an algorithm for the steepest descent method:\n",
    "\n",
    "Start with an initial guess $x_{(0)}$.\n",
    "Then, given $x_{(i)}$, compute\n",
    "$$\n",
    "   \\begin{aligned}\n",
    "      r_{(i)} &= b - Ax_{(i)} \\\\\n",
    "      \\alpha_{(i)} &= \\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T A r_{(i)}} \\\\\n",
    "      x_{(i+1)} &= x_{(i)} + \\alpha_{(i)} r_{(i)}\n",
    "   \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence analysis of steepest descent\n",
    "\n",
    "The main tool we will use to analyze iterative methods like steepest descent (and later conjugate gradients) will be eigenvectors and eigenvalues.\n",
    "\n",
    "We first recall some facts of the the eigenvalues and eigenvectors of SPD matrices:\n",
    "\n",
    "* The eigenvalues of symmetric matrices are real\n",
    "* The eigenvalues of SPD matrices are positive\n",
    "* The eigenvectors of a symmetric matrix form an orthonormal basis\n",
    "\n",
    "What happens if at some point in steepest descent the error $e_{(i)}$ is an eigenvector?\n",
    "In this case, the residual is **also** an eigenvector since\n",
    "$$\n",
    "   r_{(i)} = -A e_{(i)} = -\\lambda _{(i)}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   x_{(i+1)} &= x_{(i)} + \\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T A r_{(i)}} r_{(i)} \\\\\n",
    "      &= x_{(i)} - \\frac{1}{\\lambda}\\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T r_{(i)}} \\lambda e_{(i)} \\\\\n",
    "      &= x_{(i)} - e_{(i)} \\\\\n",
    "      &= x,\n",
    "\\end{aligned}\n",
    "$$\n",
    "and so in this case, steepest descent will converge immediately.\n",
    "\n",
    "\n",
    "This is clearly a very special case, and so if we want to consider the more general case, we can express $e_{(i)}$ as a linear combination of eigenvectors (recall that they form an orthonormal basis),\n",
    "$$\n",
    "   e_{(i)} = \\sum_{j=1}^n \\xi_j v_j.\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   r_{(i)} &= -A e_{(i)} = -\\sum_j \\xi_j \\lambda_j v_j \\\\\n",
    "   \\| e_{(i)} \\|^2 &= e_{(i)}^T e_{(i)} = \\sum_j \\xi_j^2 \\\\\n",
    "   e_{(i)}^T A e_{(i)} &= \\sum_j \\xi_j^2 \\lambda_j \\\\\n",
    "   \\| r_{(i)} \\|^2 &= r_{(i)}^T r_{(i)} = \\sum_j \\xi_j^2 \\lambda_j^2 \\\\\n",
    "   r_{(i)}^T A r_{(i)} &= \\sum_j \\xi_j^2 \\lambda_j^3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This means that the error at the next step is given by\n",
    "$$\n",
    "   e_{(i+1)}\n",
    "      = e_{(i)} + \\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T A r_{(i)}} r_{(i)}\n",
    "      = e_{(i)} + \\frac{ \\sum_j \\xi_j^2 \\lambda_j^2 }{\\sum_j \\xi_j^2 \\lambda_j^3} r_{(i)}\n",
    "$$\n",
    "\n",
    "If all the eigenvectors happen to have the same eigenvalue $\\lambda$ (again, a **very special case**), then\n",
    "$$\n",
    "   e_{(i+1)} = e_{(i)} + \\frac{ \\lambda^2 \\sum_j \\xi_j^2 }{\\lambda^3 \\sum_j \\xi_j^2} r_{(i)},\n",
    "$$\n",
    "and we again have immediate convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze the general case in the $A$-norm (also called the energy norm — why, and what is the connection with the energy norm from finite elements?)\n",
    "$$\n",
    "   \\| e \\|_A = \\left( e^T A e \\right)^{1/2}.\n",
    "$$\n",
    "We calculate\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\| e_{(i+1)} \\|_A^2\n",
    "      &= e_{(i+1)}^T A e_{(i+1)} \\\\\n",
    "      &= (e_{(i)} + \\alpha_{(i)}r_{(i)})^T A (e_{(i)} + \\alpha_{(i)} r_{(i)}) \\\\\n",
    "      &= e_{(i)}^T A e_{(i)} + 2\\alpha r_{(i)}^T A e_{(i)} + \\alpha_{(i)}^2 r_{(i)}^T A r _{(i)} \\\\\n",
    "      &= \\| e_{(i)} \\|_A^2 - 2\\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T A r_{(i)}}(r_{(i)}^T r_{(i)}) + \\left( \\frac{r_{(i)}^T r_{(i)}}{r_{(i)}^T A r_{(i)}} \\right)^2 r_{(i)}^T A r_{(i)} \\\\\n",
    "      &= \\| e_{(i)} \\|_A^2 - \\frac{(r_{(i)}^T r_{(i)})^2}{r_{(i)}^T A r_{(i)}} \\\\\n",
    "      &= \\| e_{(i)} \\|_A^2 \\left(1  - \\frac{(r_{(i)}^T r_{(i)})^2}{r_{(i)}^T A r_{(i)} e_{(i)}^T A e_{(i)}} \\right) \\\\\n",
    "      &= \\| e_{(i)} \\|_A^2 \\left(1  - \\frac{ (\\sum_j \\xi_j^2 \\lambda_j^2)^2 }{ ( \\sum_j \\xi_j^2 \\lambda_j^3 ) (\\sum_j \\xi_j^2 \\lambda_j)} \\right) \\\\\n",
    "      &= \\| e_{(i)} \\|_A^2 \\omega^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\omega^2$ is defined by\n",
    "$$\n",
    "   \\omega^2 = 1  - \\frac{ (\\sum_j \\xi_j^2 \\lambda_j^2)^2 }{ ( \\sum_j \\xi_j^2 \\lambda_j^3 ) (\\sum_j \\xi_j^2 \\lambda_j)}\n",
    "$$\n",
    "\n",
    "We will estimate $\\omega^2$ in the case where $n = 2$.\n",
    "Later we will provide a prove that this estimate extends to the case of general $n$.\n",
    "\n",
    "We have two eigenvalues, and assume $\\lambda_1 \\geq \\lambda_2$.\n",
    "Then,\n",
    "$$\n",
    "   e_{(i)} = \\xi_1 v_1 + \\xi_2 v_2.\n",
    "$$\n",
    "Let\n",
    "$$\n",
    "   \\kappa = \\frac{\\lambda_1}{\\lambda_2}, \\qquad\n",
    "   \\mu = \\frac{\\xi_2}{\\xi_1}.\n",
    "$$\n",
    "(Note that $\\kappa \\geq 1$ always).\n",
    "Then,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\omega^2\n",
    "      &= 1  - \\frac{ (\\xi_1^2\\lambda_1^2 + \\xi_2^2\\lambda_2^2)^2 }{(\\xi_1^2\\lambda_1 + \\xi_2^2\\lambda_2)(\\xi_1^2\\lambda_1^3 + \\xi_2^2\\lambda_2^3)} \\\\\n",
    "      &= 1 - \\frac{ (\\kappa^2 + \\mu^2)^2 }{ (\\kappa + \\mu^2)(\\kappa^3 + \\mu^2)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Some tedious calculations show that $\\omega$ is maximized when $\\mu = \\pm \\kappa$, which gives the inequality\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\omega^2\n",
    "      &\\leq 1 - \\frac{4\\kappa^4}{\\kappa^5 + 2\\kappa^4 + \\kappa^3} \\\\\n",
    "      &= \\frac{\\kappa^5 - 2\\kappa^4 + \\kappa^3}{\\kappa^5 + 2\\kappa^4 + \\kappa^3} \\\\\n",
    "      &= \\frac{(\\kappa - 1)^2}{(\\kappa + 1)^2},\n",
    "\\end{aligned}\n",
    "$$\n",
    "and so\n",
    "$$\n",
    "   \\omega \\leq \\frac{\\kappa - 1}{\\kappa + 1}.\n",
    "$$\n",
    "When $\\kappa$ is small (very close to 1), $\\omega$ is small, leading to fast convergence.\n",
    "When $\\kappa$ is large, $\\omega$ is very close to 1, leading to slow convergence.\n",
    "\n",
    "In general, we define the **condition number** of a SPD matrix by\n",
    "$$\n",
    "   \\kappa = \\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{min}}},\n",
    "$$\n",
    "and obtain the convergence result for steepest descent\n",
    "$$\n",
    "   \\| e_{(i)} \\|_A \\leq \\left( \\frac{\\kappa - 1}{\\kappa + 1} \\right)^i \\| e_{(0)} \\|_A\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "title": "Lecture 7",
  "vscode": {
   "interpreter": {
    "hash": "7b872a469691ea968089b37a03c3efa983bcf0e5d816bff1f8d732325b78f6a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
