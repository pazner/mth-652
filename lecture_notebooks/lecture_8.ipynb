{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH 652: Advanced Numerical Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "d5sMJY4v8Tol"
   },
   "source": [
    "### Topics\n",
    "\n",
    "* Solvers and numerical linear algebra\n",
    "* Conjugate gradient method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method of conjugate directions\n",
    "\n",
    "One downside of steepest descent is that it tends to repeat search directions over and over again.\n",
    "In fact, in 2D, it is easy to see that steepest descent will always use only two search directions at right angles to each other.\n",
    "Somehow this seems wasteful: we return to the same search directions over and over again, and so the line search isn't really getting us to the best place along the line from the point of view of convergence.\n",
    "\n",
    "What if we have a set of orthogonal search directions, and we can use each search direction **only once**, so that each step will take us to exactly the right place along that search direction.\n",
    "\n",
    "In other words, we have a set of directions\n",
    "$$\n",
    "   \\{ d_{(0)}, d_{(1)}, \\ldots, d_{(n-1)} \\},\n",
    "$$\n",
    "and at each step we choose a point\n",
    "$$\n",
    "   x_{(i+1)} = x_{(i)} + \\alpha_{(i)} d_{(i)}.\n",
    "$$\n",
    "\n",
    "If the error $e_{(i+1)}$ is **orthogonal** to $d_{(i)}$, then that means that the error can be expressed as a linear combination of the **other** search directions.\n",
    "In other words, we never need to step in the direction $d_{(i)}$ again.\n",
    "This condition can be expressed as\n",
    "$$\n",
    "   \\begin{aligned}\n",
    "      d_{(i)}^T e_{(i+1)} &= 0 \\\\\n",
    "      d_{(i)}^T ( e_{(i)} + \\alpha_{(i)} d_{(i)} ) &= 0 \\\\\n",
    "      \\alpha_{(i)} &= -\\frac{d_{(i)}^T e_{(i)}}{d_{(i)}^T d_{(i)}}\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "Unfortunately, $\\alpha_{(i)}$ isn't really computable using this expression: it depends upon knowing $e_{(i)}$, and if we knew $e_{(i)}$, we would be done.\n",
    "\n",
    "The way around this problem is to choose search directions that, instead of being **orthogonal**, are **conjugate**, in other words, $A$-orthogonal, i.e.\n",
    "$$\n",
    "   d_{(i)}^T A d_{(j)} = 0\n",
    "$$\n",
    "whenever $i \\neq j$.\n",
    "\n",
    "If the directions are conjugate, then we want the new error $e_{(i+1)}$ to be $A$-orthogonal to the search direction, i.e.\n",
    "$$\n",
    "   \\begin{aligned}\n",
    "      d_{(i)}^T A e_{(i+1)} &= 0 \\\\\n",
    "      d_{(i)}^T A (e_{(i)} + \\alpha_{(i)} d_{(i)}) &= 0 \\\\\n",
    "      \\alpha_{(i)} = \\frac{d_{(i)}^T r_{(i)}}{d_{(i)}^T A d_{(i)}},\n",
    "   \\end{aligned}\n",
    "$$\n",
    "which **is** computable, because it involves the residual $r_{(i)} = b - Ax_{(i)}$.\n",
    "\n",
    "This is actually equivalent to a line search along this direction:\n",
    "$$\n",
    "   \\begin{aligned}\n",
    "      \\frac{d}{d\\alpha} f(x_{(i+1)}) &= 0 \\\\\n",
    "      f'(x_{(i+1)})^T \\frac{d}{d\\alpha} x_{(i+1)} &= 0 \\\\\n",
    "      -r_{(i+1)}^T d_{(i)} &= 0 \\\\\n",
    "      d_{(i)}^T A e_{(i+1)} &= 0\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "This is very similar to steepest descent, but we don't choose the search vectors according to the direction of steepest increase/decrease.\n",
    "\n",
    "This procedure is guaranteed to converge in at most $n$ steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is it guaranteed to work?\n",
    "\n",
    "Express the starting error in terms of the search directions\n",
    "$$\n",
    "   e_{(0)} = \\sum_{j=0}^{n-1} \\delta_j d_{(j)}.\n",
    "$$\n",
    "\n",
    "What are the coefficients $\\delta_j$?\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   d_{(k)}^T A e_{(0)} &= \\sum_j \\delta_j d_{(k)}^T A d_{(j)} \\\\\n",
    "   d_{(k)}^T A e_{(0)} &= \\delta_k d_{(k)}^T A d_{(k)} \\\\\n",
    "   \\delta_k &= \\frac{d_{(k)}^T A e_{(0)}}{d_{(k)}^T A d_{(k)}} \\\\\n",
    "      &= \\frac{d_{(k)}^T A (e_{(k)} - \\sum_{i=0}^{k-1} \\alpha_{(i)} d_{(i)})}{d_{(k)}^T A d_{(k)}} \\\\\n",
    "      &= \\frac{d_{(k)}^T A e_{(k)}}{d_{(k)}^T A d_{(k)}} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From this, it follows that $\\alpha_{(i)} = -\\delta_{(i)}$.\n",
    "\n",
    "This gives us another way of looking at the iterative procedure.\n",
    "At every iteration, we cut down one component of the error term:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   e_{(i)} &= e_{(0)} + \\sum_{j=0}^{i-1} \\alpha_{(j)}d_{(j)} \\\\\n",
    "      &= \\sum_{j=0}^{n-1} \\delta_{(j)} d_{(j)} - \\sum_{j=0}^{i-1} \\delta_{(j)} d_{(j)} \\\\\n",
    "      &= \\sum_{j=i}^{n-1} \\delta_{(j)} d_{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimality of the error term\n",
    "\n",
    "Recall that steepest descent returns to the same search vectors over and over again.\n",
    "In this sense, it is finding approximations that are not optimal in the space spanned by the search vectors.\n",
    "On the other hand, the methods of conjugate directions **does** find an optimal approximation in the search space.\n",
    "\n",
    "Let $\\mathcal{D}_i = \\operatorname{span}\\{ d_{(0)}, d_{(1)}, \\ldots , d_{(i-1)} \\}$\n",
    "\n",
    "The error $e_{(i)}$ lies in the space $e_{(0)} + \\mathcal{D}_i$.\n",
    "In fact, $e_{(i)}$ is the **minimum** (in the energy norm) of all vectors in this space.\n",
    "\n",
    "Note that the energy norm of $e_{(i)}$ can be written as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\| e_{(i)} \\|_A &= \\sum_{j=i}^{n-1} \\sum_{k=i}^{n-1} \\delta_{(j)} \\delta_{(k)} d_{(j)}^T A d_{(k)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "using the expression for the error derived above.\n",
    "Then, by $A$-orthogonality of the search directions,\n",
    "$$\n",
    "   \\| e_{(i)} \\|_A = \\sum_{j=i}^{n-1} \\delta_{(j)}^2 d_{(j)}^T A d_{(j)}\n",
    "$$\n",
    "\n",
    "Any **other** element of $e_{(0)} + \\mathcal{D}_i$ will involve some of the vectors $d_{(k)}$.\n",
    "Since these are set to zero in the above expansion, we know that this is the minimizer in the $A$-norm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gram-Schmidt conjugation\n",
    "\n",
    "Everything seems good about this method.\n",
    "All we need now is to find some way to generate the $A$-orthogonal (conjugate) search directions $d_{(i)}$.\n",
    "\n",
    "Suppose we have $n$ linearly independent vectors $u_0, u_1, \\ldots, u_{n-1}$.\n",
    "(We can take the standard basis vectors, for example).\n",
    "\n",
    "1. Set $d_{(0)} = u_0$.\n",
    "2. To find $d_{(i)}$, subtract off any components of $u_i$ that are not $A$-orthogonal to $d_{(0)}, \\ldots, d_{(i-1)}$\n",
    "\n",
    "That is, set\n",
    "$$\n",
    "   d_{(i)} := u_i + \\sum_{k=0}^{i-1} \\beta_{ik} d_{(k)}\n",
    "$$\n",
    "for appropriate $\\beta_{ik}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   d_{(i)}^T A d_{(j)} &= u_i^T A d_{(j)} + \\sum_{k=0}^{i-1} \\beta_{ik} d_{(k)}^T A d_{(j)} \\\\\n",
    "   0 &= u_i^T A d_{(j)} + \\beta_{ij} d_{(j)}^T A d_{(j)} \\\\\n",
    "   \\beta_{ij} &= - \\frac{u_i^T A d_{(j)}}{d_{(j)}^T A d_{(j)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In general, this procedure is expensive, because it requires storing all of the previous search directions, and it will take $\\mathcal{O}(n^3)$ operations to generate a full set of vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Method of Conjugate Gradients\n",
    "\n",
    "We can **finally** describe the conjugate gradient method.\n",
    "We just set\n",
    "$$\n",
    "   u_i = r_{(i)}.\n",
    "$$\n",
    "\n",
    "Why would we do this?\n",
    "\n",
    "First, since the search directions are $A$-orthogonal to the error, the residual must be orthogonal to all previous search directions.\n",
    "This means we will always get new, linearly independent vectors.\n",
    "\n",
    "Since the residuals are used as the search directions, we have that\n",
    "$$\n",
    "   \\mathcal{D}_i = \\operatorname{span} \\{ r_{(0)}, r_{(1)}, \\ldots, r_{(i-1)} \\}.\n",
    "$$\n",
    "\n",
    "$r_{(i)}$ is orthogonal to the search directions, so $r_{(i)}$ is orthogonal to $\\mathcal{D}_i$, and therefore $r_{(i)}$ is orthogonal to all the other residuals,\n",
    "$$\n",
    "   r_{(i)}^T r_{(j)} = 0, \\qquad i \\neq j\n",
    "$$\n",
    "\n",
    "Also, note that the residual is given by\n",
    "$$\n",
    "   r_{(i+1)} = -A e_{(i+1)} = -A(e_{(i)} + \\alpha_{(i)} d_{(i)}) = r_{(i)} - \\alpha_{(i)} A d_{(i)},\n",
    "$$\n",
    "so $r_{(i+1)}$ is a linear combination of the previous residual and $A d_{(i)}$.\n",
    "\n",
    "This means that the space $\\mathcal{D}_{i+1}$ is given by the union of $\\mathcal{D}_i$ and $A \\mathcal{D}_i$,\n",
    "so\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\mathcal{D}_i &= \\operatorname{span}\\{ d_{(0)}, Ad_{(0)}, A^2 d_{(0)}, \\ldots, A^{i-1} d_{(0)}\\} \\\\\n",
    "   &= \\operatorname{span}\\{ r_{(0)}, Ar_{(0)}, A^2 r_{(0)}, \\ldots, A^{i-1} r_{(0)}\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This type of subspace is called a **Krylov** subspace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This choice of search directions has a very important property:\n",
    "\n",
    "Note that $r_{(i+1)}$ is orthogonal to $\\mathcal{D}_{i+1}$.\n",
    "But $A \\mathcal{D}_i$ is a subspace of $\\mathcal{D}_{i+1}$.\n",
    "So, $r_{(i+1)}$ is orthogonal to $A \\mathcal{D}_i$, and hence $r_{(i+1)}$ is $A$-orthogonal to $\\mathcal{D}_i$.\n",
    "\n",
    "This means that to generate a new search direction $d_{(i+1)}$ that is $A$-orthogonal to all the previous search directions, we just need to make sure that it's orthogonal to $d_{(i)}$, since it's automatically $A$-orthogonal to the previous ones.\n",
    "\n",
    "Note that we had\n",
    "$$\n",
    "   r_{(i+1)} = r_{(i)} - \\alpha_{(i)} A d_{(i)},\n",
    "$$\n",
    "and so\n",
    "$$\n",
    "   r_{(i)}^T r_{(j+1)} = r_{(i)}^T r_{(j)} - \\alpha_{(j)} r_{(i)}^T A d_{(j)},\n",
    "$$\n",
    "therefore\n",
    "$$\n",
    "   \\alpha_{(j)} r_{(i)}^T A d_{(j)} = r_{(i)}^T r_{(j)} - r_{(i)}^T r_{(j+1)}\n",
    "$$\n",
    "\n",
    "Note that the right-hand side vanishes whenever $i$ is neither $j$ nor $j+1$.\n",
    "Returning to the formula for $\\beta_{ij}$, this means that almost all of the $\\beta_{ij}$ are now zero, and we don't need to store the old search directions to guarantee $A$-orthogonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "title": "Lecture 8",
  "vscode": {
   "interpreter": {
    "hash": "7b872a469691ea968089b37a03c3efa983bcf0e5d816bff1f8d732325b78f6a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
